#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

# AWS S3 configuration

spark.hadoop.fs.s3.impl                                        org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.endpoint                                   https://s3.amazonaws.com
spark.hadoop.fs.s3a.aws.credentials.provider                   org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider,com.amazonaws.auth.EnvironmentVariableCredentialsProvider,com.amazonaws.auth.InstanceProfileCredentialsProvider
spark.hadoop.mapreduce.fileoutputcommiter.algorithm.version    2

# Delta Lake configuration

spark.sql.extensions                                           io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog                                org.apache.spark.sql.delta.catalog.DeltaCatalog

# You can use environment variables too o set in conf below
# spark.hadoop.fs.s3a.access.key ******
# spark.hadoop.fs.s3a.secret.key ******

# Google Cloud Storage configuration
# More in:
#  https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/master/gcs/INSTALL.md
#  https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/master/gcs/CONFIGURATION.md

# The AbstractFileSystem for 'gs:' URIs
spark.hadoop.fs.AbstractFileSystem.gs.impl                     com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS

# Optional. Google Cloud Project ID with access to GCS buckets.
# Required only for list buckets and create bucket operations.
# spark.hadoop.fs.gs.project.id

# Whether to use a service account for GCS authorization. Setting this
# property to `false` will disable use of service accounts for authentication.
# spark.hadoop.google.cloud.auth.service.account.enable        true

# The JSON keyfile of the service account used for GCS
# access when google.cloud.auth.service.account.enable is true.
# spark.hadoop.google.cloud.auth.service.account.json.keyfile    /path/to/keyfile
